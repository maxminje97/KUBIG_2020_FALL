{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ch08.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOQFPMRkLOTEPKhf9XpfhrH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/minnnjecho/KUBIG_2020_FALL/blob/master/KU-BIG/2021_Spring/ch08.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfxriR08xdfk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2b194c8-e1df-4dc5-b9d9-2391e9ba78b7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYAH5VEabmL7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4e129c6-557f-4fa2-c5ee-aeb9b932514b"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "T, H = 5, 4    # 시계열의 길이: T, 은닉 상태 벡터의 원소 수: H\n",
        "hs = np.random.randn(T, H)\n",
        "print(hs)\n",
        "a = np.array([0.8, 0.1, 0.03, 0.05, 0.02]) # shape: (5, ), 가중치 벡터\n",
        "\n",
        "ar = a.reshape(5, 1).repeat(4, axis=1) # hs와의 연산을 위해 확장(실제로는 안해줘도 됨)\n",
        "print(ar)\n",
        "print(ar.shape) # shape: (5, 4)\n",
        "\n",
        "t = hs * ar # Encoder에서 받은 hs에 가중합(원소별 곱을 계산)\n",
        "print(t)\n",
        "print(t.shape)\n",
        "\n",
        "c = np.sum(t, axis=0) # 0번째 축이 사라짐.\n",
        "print(c.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.28402538  2.05846921 -0.24260861  0.76716   ]\n",
            " [-0.63235436  0.74719498  1.29141102 -1.02398598]\n",
            " [ 1.90489048 -1.34088943  1.47110725  1.49042319]\n",
            " [-0.53821012  0.46875563  0.76289666 -1.47590461]\n",
            " [ 0.68239529  0.1576866  -1.6650861  -0.44989876]]\n",
            "[[0.8  0.8  0.8  0.8 ]\n",
            " [0.1  0.1  0.1  0.1 ]\n",
            " [0.03 0.03 0.03 0.03]\n",
            " [0.05 0.05 0.05 0.05]\n",
            " [0.02 0.02 0.02 0.02]]\n",
            "(5, 4)\n",
            "[[ 0.2272203   1.64677537 -0.19408689  0.613728  ]\n",
            " [-0.06323544  0.0747195   0.1291411  -0.1023986 ]\n",
            " [ 0.05714671 -0.04022668  0.04413322  0.0447127 ]\n",
            " [-0.02691051  0.02343778  0.03814483 -0.07379523]\n",
            " [ 0.01364791  0.00315373 -0.03330172 -0.00899798]]\n",
            "(5, 4)\n",
            "(4,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbhW8xp1gr9B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d3f1419-cf34-4e70-a31e-949a11f5de12"
      },
      "source": [
        "N, T, H = 10, 5, 4\n",
        "hs = np.random.randn(N, T, H)\n",
        "a = np.random.randn(N, T)\n",
        "ar = a.reshape(N, T, 1).repeat(H, axis=2)\n",
        "# ar = a.reshape(N, T, 1) # 브로드캐스트를 사용하는 경우\n",
        "\n",
        "t = hs * ar\n",
        "print(t.shape)\n",
        "\n",
        "c = np.sum(t, axis=1)\n",
        "print(c.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10, 5, 4)\n",
            "(10, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGrsKRjXgr_l"
      },
      "source": [
        "class WeightSum:\n",
        "    def __init__(self):\n",
        "        self.params, self.grads = [], []\n",
        "        self.cache = None\n",
        "    \n",
        "    def forward(self, hs, a):\n",
        "        N, T, H = hs.shape\n",
        "\n",
        "        ar = a.reshape(N, T, 1).repeat(H, axis=2)\n",
        "        t = hs * ar\n",
        "        c = np.sum(t, axis=1)\n",
        "\n",
        "        self.cache = (hs, ar)\n",
        "        return c\n",
        "    \n",
        "    def backward(self, dc):\n",
        "        hs, ar = self.cache\n",
        "        N, T, H = hs.shape\n",
        "\n",
        "        dt = dc.reshape(N, 1, H).repeat(T, axis=1) # sum의 역전파\n",
        "        dar = dt * hs\n",
        "        dhs = dt * ar\n",
        "        da = np.sum(dar, axis=2) # repeat의 역전파\n",
        "\n",
        "        return dhs, da"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoaJdfcEikeX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85756133-7c03-41d3-957e-2dd6873f539f"
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/Shareddrives/KU-BIG_2021-1_NLP_Study/input')\n",
        "from common.layers import Softmax\n",
        "import numpy as np\n",
        "\n",
        "N, T, H = 10, 5, 4\n",
        "hs = np.random.randn(N, T, H)\n",
        "h = np.random.randn(N, H)\n",
        "hr = h.reshape(N, 1, H).repeat(T, axis=1)\n",
        "# hr = h.reshape(N, 1, H) # 브로드캐스트를 사용하는 경우\n",
        "\n",
        "t = hs * hr\n",
        "print(t.shape) # (10, 5, 4)\n",
        "\n",
        "s = np.sum(t, axis=2)\n",
        "print(s.shape)\n",
        "\n",
        "softmax = Softmax()\n",
        "a = softmax.forward(s)\n",
        "print(a.shape) # (10, 5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10, 5, 4)\n",
            "(10, 5)\n",
            "(10, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUNr_-Ty3hPx"
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/Shareddrives/KU-BIG_2021-1_NLP_Study/input')\n",
        "from common.np import *\n",
        "from common.layers import Softmax\n",
        "\n",
        "class AttentionWeight:\n",
        "    def __init__(self):\n",
        "        self.params, self.grads = [], []\n",
        "        self.softmax = Softmax()\n",
        "        self.cache = none\n",
        "    \n",
        "    def forward(self, hs, h):\n",
        "        N, T, H = hs.shape\n",
        "\n",
        "        hr = h.reshape(N, 1, H).repeat(T, axis=1)\n",
        "        t = hs * hr\n",
        "        s = np.sum(t, axis=2)\n",
        "        a = self.softmax.forward(s)\n",
        "\n",
        "        self.cache = (hs, hr)\n",
        "        return a\n",
        "\n",
        "    def backward(self, da):\n",
        "        hs, hr = self.cache\n",
        "        N, T, H = hs.shape\n",
        "\n",
        "        ds = self.softmax.backward(da)\n",
        "        dt = ds.reshape(N, T, 1).repeat(H, axis=2)\n",
        "        dhs = dt * hr\n",
        "        dhr = dt * hs\n",
        "        dh = np.sum(dhr, axis=1)\n",
        "\n",
        "        return dhs, dh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCiv6an6tnXu"
      },
      "source": [
        "class Attention:\n",
        "    def __init__(self):\n",
        "        self.params, self.grads = [], []\n",
        "        self.attention_weight_layer = AttentionWeight()\n",
        "        self.weight_sum_layer = WeightSum()\n",
        "        self.attention_weight = None\n",
        "\n",
        "    def forward(self, hs, h):\n",
        "        a = self.attention_weight_layer.forward(hs, h)\n",
        "        out = self.weight_sum_layer.forward(hs, a)\n",
        "        self.attention_weight = a\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dhs0, da = self.weight_sum_layer.backward(dout)     # AttentionWeight()의 backward\n",
        "        dhs1, dh = self.attention_weight_layer.backward(da) # WeightSum()의 backward\n",
        "        dhs = dhs0 + dhs1\n",
        "        return dhs, dh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8u7YtKztnv-"
      },
      "source": [
        "class TimeAttention:\n",
        "    def __init__(self):\n",
        "        self.params, self.grads = [], []\n",
        "        self.layers = None\n",
        "        self.attention_weights = None\n",
        "\n",
        "    def forward(self, hs_enc, hs_dec):\n",
        "        N, T, H = hs_dec.shape\n",
        "        out = np.empty_like(hs_dec)\n",
        "        self.layers = []\n",
        "        self.attention_weights = []\n",
        "\n",
        "        for t in range(T):\n",
        "            layer = Attention()\n",
        "            out[:, t, :] = layer.forward(hs_enc, hs_dec[:, t, :])\n",
        "            self.layers.append(layer)\n",
        "            self.attention_weights.append(layer.attention_weight)\n",
        "        \n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        N, T, H = dout.shape\n",
        "        dhs_enc = 0\n",
        "        dhs_dec = np.empty_like(dout)\n",
        "\n",
        "        for t in range(T):\n",
        "            layer = self.layers[t]\n",
        "            dhs, dh = layer.backward(dout[:, t, :]) \n",
        "            dhs_enc += dhs\n",
        "            dhs_dec[:, t, :] = h\n",
        "        \n",
        "        return dhs_enc, dhs_dec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wj4ILzL26wJf"
      },
      "source": [
        "## 어텐션을 갖춘 seq2seq 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_R0YkZS_y9h"
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/Shareddrives/KU-BIG_2021-1_NLP_Study/input')\n",
        "from common.time_layers import *\n",
        "from ch07.seq2seq import Encoder, Seq2seq\n",
        "from ch08.attention_layer import TimeAttention\n",
        "\n",
        "class AttentionEncoder(Encoder):\n",
        "    def forward(self, xs):\n",
        "        xs = self.embed.forward(xs)\n",
        "        hs = self.lstm.forward(xs)\n",
        "        return hs\n",
        "    \n",
        "    def backward(self, dhs):\n",
        "        dout = self.lstm.backward(dhs)\n",
        "        dout = self.embed.backward(dout)\n",
        "        return dout"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VN5h-oZc7RsQ"
      },
      "source": [
        "class AttentionDecoder:\n",
        "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
        "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
        "        rn = np.random.randn\n",
        "        embed_W = (rn(V, D) / 100).astype('f')\n",
        "        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
        "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
        "        lstm_b = np.zeros(4 * H).astype('f')\n",
        "        affine_W = (rn(2 * H, V) / np.sqrt(2 * H)).astype('f')\n",
        "        affine_b = np.zeros(V).astype('f')\n",
        "\n",
        "\n",
        "        self.embed = TimeEmbedding(embed_W)\n",
        "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)\n",
        "        self.attention = TimeAttnetion()\n",
        "        self.affine = TimeAffine(affine_W, affine_b)\n",
        "        layers = [self.embed, self.lstm, self.attention, self.affine]\n",
        "\n",
        "\n",
        "        self.params, self.grads = [], []\n",
        "        for layer in layers:\n",
        "            self.params += layers.params\n",
        "            self.grads += layer.grads\n",
        "    \n",
        "\n",
        "    def forward(self, xs, enc_hs):\n",
        "        h = enc_hs[:, -1]\n",
        "        self.lstm.set_state(h)\n",
        "\n",
        "\n",
        "        out = self.embed.forward(xs)\n",
        "        dec_hs = self.lstm.forward(out)\n",
        "        c = self.attention.forward(enc_hs, dec_hs)\n",
        "        out = np.concatenate((c, dec_hs), axis=2)\n",
        "        score = self.affine.forward(out)\n",
        "\n",
        "        return score\n",
        "    \n",
        "    # def backward(self, dscore):\n",
        "    #     # 깃허브의 소스 코드 참고\n",
        "        \n",
        "    # def generate(self, enc_hs, start_id, sample_size):\n",
        "    #     # 깃허브의 소스 코드 참고"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVIgnUJS9kp1"
      },
      "source": [
        "from ch07.seq2seq import Encoder, Seq2seq\n",
        "\n",
        "class AttentionSeq2seq(Seq2seq):\n",
        "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
        "        args = vocab_size, wordvec_size, hidden_size\n",
        "        self.encoder = AttentionEncoder(*args)\n",
        "        self.decoder = AttentionDecoder(*args)\n",
        "        self.softmax = TimeSoftmaxWithLoss()\n",
        "\n",
        "\n",
        "        self.params = self.encoder.params + self.decoder.params\n",
        "        self.grads = self.encoder.grads + self.decoder.grads"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfywwG-dF9i5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b8334bd-45a4-410b-eb9a-9f8356784304"
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/Shareddrives/KU-BIG_2021-1_NLP_Study/input')\n",
        "sys.path.append('/content/drive/Shareddrives/KU-BIG_2021-1_NLP_Study/input/ch07')\n",
        "import numpy as np\n",
        "from dataset import sequence\n",
        "from common.optimizer import Adam\n",
        "from common.trainer import Trainer\n",
        "from common.util import eval_seq2seq\n",
        "from ch08.attention_seq2seq import AttentionSeq2seq\n",
        "from ch07.seq2seq import Seq2seq\n",
        "from ch07.peeky_seq2seq import PeekySeq2seq\n",
        "\n",
        "# 데이터 읽기\n",
        "(x_train, t_train), (x_test, t_test) = sequence.load_data('date.txt')\n",
        "char_to_id, id_to_char = sequence.get_vocab()\n",
        "\n",
        "# 입력 문장 반전\n",
        "x_train, x_test = x_train[:, ::-1], x_test[:, ::-1]\n",
        "\n",
        "# 하이퍼파라미터 설절\n",
        "vocab_size = len(char_to_id)\n",
        "wordvec_size = 16\n",
        "hidden_size = 256\n",
        "batch_size = 128\n",
        "max_epoch = 10\n",
        "max_grad = 5\n",
        "\n",
        "model = AttentionSeq2seq(vocab_size, wordvec_size, hidden_size)\n",
        "optimizer = Adam()\n",
        "trainer = Trainer(model, optimizer)\n",
        "\n",
        "acc_list = []\n",
        "for epoch in range(max_epoch):\n",
        "    trainer.fit(x_train, t_train, max_epoch=1,\n",
        "                batch_size=batch_size, max_grad=max_grad)\n",
        "    \n",
        "    correct_num = 0\n",
        "    for i in range(len(x_test)):\n",
        "        question, correct = x_test[[i]], t_test[[i]]\n",
        "        verbose = i < 10\n",
        "        correct_num += eval_seq2seq(model, question, correct,\n",
        "                                    id_to_char, verbose, is_reverse=True)\n",
        "    \n",
        "    acc = float(correct_num) / len(x_test)\n",
        "    acc_list.append(acc)\n",
        "    print('val acc %.3f%%' %(acc*100))\n",
        "\n",
        "model.save_params()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| 에폭 1 |  반복 1 / 351 | 시간 0[s] | 손실 4.08\n",
            "| 에폭 1 |  반복 21 / 351 | 시간 11[s] | 손실 3.09\n",
            "| 에폭 1 |  반복 41 / 351 | 시간 23[s] | 손실 1.90\n",
            "| 에폭 1 |  반복 61 / 351 | 시간 35[s] | 손실 1.72\n",
            "| 에폭 1 |  반복 81 / 351 | 시간 47[s] | 손실 1.46\n",
            "| 에폭 1 |  반복 101 / 351 | 시간 59[s] | 손실 1.19\n",
            "| 에폭 1 |  반복 121 / 351 | 시간 71[s] | 손실 1.14\n",
            "| 에폭 1 |  반복 141 / 351 | 시간 84[s] | 손실 1.09\n",
            "| 에폭 1 |  반복 161 / 351 | 시간 96[s] | 손실 1.06\n",
            "| 에폭 1 |  반복 181 / 351 | 시간 108[s] | 손실 1.04\n",
            "| 에폭 1 |  반복 201 / 351 | 시간 120[s] | 손실 1.03\n",
            "| 에폭 1 |  반복 221 / 351 | 시간 132[s] | 손실 1.02\n",
            "| 에폭 1 |  반복 241 / 351 | 시간 144[s] | 손실 1.02\n",
            "| 에폭 1 |  반복 261 / 351 | 시간 156[s] | 손실 1.01\n",
            "| 에폭 1 |  반복 281 / 351 | 시간 168[s] | 손실 1.00\n",
            "| 에폭 1 |  반복 301 / 351 | 시간 181[s] | 손실 1.00\n",
            "| 에폭 1 |  반복 321 / 351 | 시간 193[s] | 손실 1.00\n",
            "| 에폭 1 |  반복 341 / 351 | 시간 205[s] | 손실 1.00\n",
            "Q 10/15/94                     \n",
            "T 1994-10-15\n",
            "\u001b[91m☒\u001b[0m 1978-08-11\n",
            "---\n",
            "Q thursday, november 13, 2008  \n",
            "T 2008-11-13\n",
            "\u001b[91m☒\u001b[0m 1978-08-11\n",
            "---\n",
            "Q Mar 25, 2003                 \n",
            "T 2003-03-25\n",
            "\u001b[91m☒\u001b[0m 1978-08-11\n",
            "---\n",
            "Q Tuesday, November 22, 2016   \n",
            "T 2016-11-22\n",
            "\u001b[91m☒\u001b[0m 1978-08-11\n",
            "---\n",
            "Q Saturday, July 18, 1970      \n",
            "T 1970-07-18\n",
            "\u001b[91m☒\u001b[0m 1978-08-11\n",
            "---\n",
            "Q october 6, 1992              \n",
            "T 1992-10-06\n",
            "\u001b[91m☒\u001b[0m 1978-08-11\n",
            "---\n",
            "Q 8/23/08                      \n",
            "T 2008-08-23\n",
            "\u001b[91m☒\u001b[0m 1978-08-11\n",
            "---\n",
            "Q 8/30/07                      \n",
            "T 2007-08-30\n",
            "\u001b[91m☒\u001b[0m 1978-08-11\n",
            "---\n",
            "Q 10/28/13                     \n",
            "T 2013-10-28\n",
            "\u001b[91m☒\u001b[0m 1978-08-11\n",
            "---\n",
            "Q sunday, november 6, 2016     \n",
            "T 2016-11-06\n",
            "\u001b[91m☒\u001b[0m 1978-08-11\n",
            "---\n",
            "val acc 0.000%\n",
            "| 에폭 2 |  반복 1 / 351 | 시간 0[s] | 손실 1.00\n",
            "| 에폭 2 |  반복 21 / 351 | 시간 12[s] | 손실 1.00\n",
            "| 에폭 2 |  반복 41 / 351 | 시간 25[s] | 손실 0.99\n",
            "| 에폭 2 |  반복 61 / 351 | 시간 38[s] | 손실 0.99\n",
            "| 에폭 2 |  반복 81 / 351 | 시간 50[s] | 손실 0.99\n",
            "| 에폭 2 |  반복 101 / 351 | 시간 63[s] | 손실 0.99\n",
            "| 에폭 2 |  반복 121 / 351 | 시간 75[s] | 손실 0.99\n",
            "| 에폭 2 |  반복 141 / 351 | 시간 88[s] | 손실 0.98\n",
            "| 에폭 2 |  반복 161 / 351 | 시간 100[s] | 손실 0.98\n",
            "| 에폭 2 |  반복 181 / 351 | 시간 112[s] | 손실 0.97\n",
            "| 에폭 2 |  반복 201 / 351 | 시간 124[s] | 손실 0.95\n",
            "| 에폭 2 |  반복 221 / 351 | 시간 137[s] | 손실 0.94\n",
            "| 에폭 2 |  반복 241 / 351 | 시간 149[s] | 손실 0.90\n",
            "| 에폭 2 |  반복 261 / 351 | 시간 161[s] | 손실 0.83\n",
            "| 에폭 2 |  반복 281 / 351 | 시간 173[s] | 손실 0.74\n",
            "| 에폭 2 |  반복 301 / 351 | 시간 185[s] | 손실 0.66\n",
            "| 에폭 2 |  반복 321 / 351 | 시간 197[s] | 손실 0.58\n",
            "| 에폭 2 |  반복 341 / 351 | 시간 210[s] | 손실 0.47\n",
            "Q 10/15/94                     \n",
            "T 1994-10-15\n",
            "\u001b[92m☑\u001b[0m 1994-10-15\n",
            "---\n",
            "Q thursday, november 13, 2008  \n",
            "T 2008-11-13\n",
            "\u001b[91m☒\u001b[0m 2006-11-13\n",
            "---\n",
            "Q Mar 25, 2003                 \n",
            "T 2003-03-25\n",
            "\u001b[92m☑\u001b[0m 2003-03-25\n",
            "---\n",
            "Q Tuesday, November 22, 2016   \n",
            "T 2016-11-22\n",
            "\u001b[92m☑\u001b[0m 2016-11-22\n",
            "---\n",
            "Q Saturday, July 18, 1970      \n",
            "T 1970-07-18\n",
            "\u001b[92m☑\u001b[0m 1970-07-18\n",
            "---\n",
            "Q october 6, 1992              \n",
            "T 1992-10-06\n",
            "\u001b[92m☑\u001b[0m 1992-10-06\n",
            "---\n",
            "Q 8/23/08                      \n",
            "T 2008-08-23\n",
            "\u001b[92m☑\u001b[0m 2008-08-23\n",
            "---\n",
            "Q 8/30/07                      \n",
            "T 2007-08-30\n",
            "\u001b[91m☒\u001b[0m 2007-08-09\n",
            "---\n",
            "Q 10/28/13                     \n",
            "T 2013-10-28\n",
            "\u001b[91m☒\u001b[0m 1983-10-28\n",
            "---\n",
            "Q sunday, november 6, 2016     \n",
            "T 2016-11-06\n",
            "\u001b[91m☒\u001b[0m 2016-11-08\n",
            "---\n",
            "val acc 51.320%\n",
            "| 에폭 3 |  반복 1 / 351 | 시간 0[s] | 손실 0.35\n",
            "| 에폭 3 |  반복 21 / 351 | 시간 12[s] | 손실 0.30\n",
            "| 에폭 3 |  반복 41 / 351 | 시간 24[s] | 손실 0.21\n",
            "| 에폭 3 |  반복 61 / 351 | 시간 36[s] | 손실 0.14\n",
            "| 에폭 3 |  반복 81 / 351 | 시간 48[s] | 손실 0.09\n",
            "| 에폭 3 |  반복 101 / 351 | 시간 60[s] | 손실 0.07\n",
            "| 에폭 3 |  반복 121 / 351 | 시간 72[s] | 손실 0.05\n",
            "| 에폭 3 |  반복 141 / 351 | 시간 84[s] | 손실 0.04\n",
            "| 에폭 3 |  반복 161 / 351 | 시간 95[s] | 손실 0.03\n",
            "| 에폭 3 |  반복 181 / 351 | 시간 107[s] | 손실 0.03\n",
            "| 에폭 3 |  반복 201 / 351 | 시간 119[s] | 손실 0.02\n",
            "| 에폭 3 |  반복 221 / 351 | 시간 131[s] | 손실 0.02\n",
            "| 에폭 3 |  반복 241 / 351 | 시간 143[s] | 손실 0.02\n",
            "| 에폭 3 |  반복 261 / 351 | 시간 154[s] | 손실 0.01\n",
            "| 에폭 3 |  반복 281 / 351 | 시간 166[s] | 손실 0.01\n",
            "| 에폭 3 |  반복 301 / 351 | 시간 178[s] | 손실 0.01\n",
            "| 에폭 3 |  반복 321 / 351 | 시간 189[s] | 손실 0.01\n",
            "| 에폭 3 |  반복 341 / 351 | 시간 201[s] | 손실 0.01\n",
            "Q 10/15/94                     \n",
            "T 1994-10-15\n",
            "\u001b[92m☑\u001b[0m 1994-10-15\n",
            "---\n",
            "Q thursday, november 13, 2008  \n",
            "T 2008-11-13\n",
            "\u001b[92m☑\u001b[0m 2008-11-13\n",
            "---\n",
            "Q Mar 25, 2003                 \n",
            "T 2003-03-25\n",
            "\u001b[92m☑\u001b[0m 2003-03-25\n",
            "---\n",
            "Q Tuesday, November 22, 2016   \n",
            "T 2016-11-22\n",
            "\u001b[92m☑\u001b[0m 2016-11-22\n",
            "---\n",
            "Q Saturday, July 18, 1970      \n",
            "T 1970-07-18\n",
            "\u001b[92m☑\u001b[0m 1970-07-18\n",
            "---\n",
            "Q october 6, 1992              \n",
            "T 1992-10-06\n",
            "\u001b[92m☑\u001b[0m 1992-10-06\n",
            "---\n",
            "Q 8/23/08                      \n",
            "T 2008-08-23\n",
            "\u001b[92m☑\u001b[0m 2008-08-23\n",
            "---\n",
            "Q 8/30/07                      \n",
            "T 2007-08-30\n",
            "\u001b[92m☑\u001b[0m 2007-08-30\n",
            "---\n",
            "Q 10/28/13                     \n",
            "T 2013-10-28\n",
            "\u001b[92m☑\u001b[0m 2013-10-28\n",
            "---\n",
            "Q sunday, november 6, 2016     \n",
            "T 2016-11-06\n",
            "\u001b[92m☑\u001b[0m 2016-11-06\n",
            "---\n",
            "val acc 99.900%\n",
            "| 에폭 4 |  반복 1 / 351 | 시간 0[s] | 손실 0.01\n",
            "| 에폭 4 |  반복 21 / 351 | 시간 12[s] | 손실 0.01\n",
            "| 에폭 4 |  반복 41 / 351 | 시간 23[s] | 손실 0.01\n",
            "| 에폭 4 |  반복 61 / 351 | 시간 35[s] | 손실 0.01\n",
            "| 에폭 4 |  반복 81 / 351 | 시간 47[s] | 손실 0.01\n",
            "| 에폭 4 |  반복 101 / 351 | 시간 58[s] | 손실 0.01\n",
            "| 에폭 4 |  반복 121 / 351 | 시간 70[s] | 손실 0.00\n",
            "| 에폭 4 |  반복 141 / 351 | 시간 82[s] | 손실 0.01\n",
            "| 에폭 4 |  반복 161 / 351 | 시간 93[s] | 손실 0.00\n",
            "| 에폭 4 |  반복 181 / 351 | 시간 105[s] | 손실 0.00\n",
            "| 에폭 4 |  반복 201 / 351 | 시간 117[s] | 손실 0.00\n",
            "| 에폭 4 |  반복 221 / 351 | 시간 129[s] | 손실 0.00\n",
            "| 에폭 4 |  반복 241 / 351 | 시간 140[s] | 손실 0.00\n",
            "| 에폭 4 |  반복 261 / 351 | 시간 152[s] | 손실 0.00\n",
            "| 에폭 4 |  반복 281 / 351 | 시간 164[s] | 손실 0.00\n",
            "| 에폭 4 |  반복 301 / 351 | 시간 176[s] | 손실 0.00\n",
            "| 에폭 4 |  반복 321 / 351 | 시간 188[s] | 손실 0.00\n",
            "| 에폭 4 |  반복 341 / 351 | 시간 200[s] | 손실 0.00\n",
            "Q 10/15/94                     \n",
            "T 1994-10-15\n",
            "\u001b[92m☑\u001b[0m 1994-10-15\n",
            "---\n",
            "Q thursday, november 13, 2008  \n",
            "T 2008-11-13\n",
            "\u001b[92m☑\u001b[0m 2008-11-13\n",
            "---\n",
            "Q Mar 25, 2003                 \n",
            "T 2003-03-25\n",
            "\u001b[92m☑\u001b[0m 2003-03-25\n",
            "---\n",
            "Q Tuesday, November 22, 2016   \n",
            "T 2016-11-22\n",
            "\u001b[92m☑\u001b[0m 2016-11-22\n",
            "---\n",
            "Q Saturday, July 18, 1970      \n",
            "T 1970-07-18\n",
            "\u001b[92m☑\u001b[0m 1970-07-18\n",
            "---\n",
            "Q october 6, 1992              \n",
            "T 1992-10-06\n",
            "\u001b[92m☑\u001b[0m 1992-10-06\n",
            "---\n",
            "Q 8/23/08                      \n",
            "T 2008-08-23\n",
            "\u001b[92m☑\u001b[0m 2008-08-23\n",
            "---\n",
            "Q 8/30/07                      \n",
            "T 2007-08-30\n",
            "\u001b[92m☑\u001b[0m 2007-08-30\n",
            "---\n",
            "Q 10/28/13                     \n",
            "T 2013-10-28\n",
            "\u001b[92m☑\u001b[0m 2013-10-28\n",
            "---\n",
            "Q sunday, november 6, 2016     \n",
            "T 2016-11-06\n",
            "\u001b[92m☑\u001b[0m 2016-11-06\n",
            "---\n",
            "val acc 99.900%\n",
            "| 에폭 5 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
            "| 에폭 5 |  반복 21 / 351 | 시간 12[s] | 손실 0.00\n",
            "| 에폭 5 |  반복 41 / 351 | 시간 25[s] | 손실 0.00\n",
            "| 에폭 5 |  반복 61 / 351 | 시간 37[s] | 손실 0.00\n",
            "| 에폭 5 |  반복 81 / 351 | 시간 49[s] | 손실 0.00\n",
            "| 에폭 5 |  반복 101 / 351 | 시간 61[s] | 손실 0.00\n",
            "| 에폭 5 |  반복 121 / 351 | 시간 73[s] | 손실 0.00\n",
            "| 에폭 5 |  반복 141 / 351 | 시간 85[s] | 손실 0.00\n",
            "| 에폭 5 |  반복 161 / 351 | 시간 97[s] | 손실 0.00\n",
            "| 에폭 5 |  반복 181 / 351 | 시간 109[s] | 손실 0.02\n",
            "| 에폭 5 |  반복 201 / 351 | 시간 121[s] | 손실 0.01\n",
            "| 에폭 5 |  반복 221 / 351 | 시간 133[s] | 손실 0.01\n",
            "| 에폭 5 |  반복 241 / 351 | 시간 145[s] | 손실 0.00\n",
            "| 에폭 5 |  반복 261 / 351 | 시간 157[s] | 손실 0.00\n",
            "| 에폭 5 |  반복 281 / 351 | 시간 169[s] | 손실 0.00\n",
            "| 에폭 5 |  반복 301 / 351 | 시간 181[s] | 손실 0.00\n",
            "| 에폭 5 |  반복 321 / 351 | 시간 193[s] | 손실 0.00\n",
            "| 에폭 5 |  반복 341 / 351 | 시간 205[s] | 손실 0.00\n",
            "Q 10/15/94                     \n",
            "T 1994-10-15\n",
            "\u001b[92m☑\u001b[0m 1994-10-15\n",
            "---\n",
            "Q thursday, november 13, 2008  \n",
            "T 2008-11-13\n",
            "\u001b[92m☑\u001b[0m 2008-11-13\n",
            "---\n",
            "Q Mar 25, 2003                 \n",
            "T 2003-03-25\n",
            "\u001b[92m☑\u001b[0m 2003-03-25\n",
            "---\n",
            "Q Tuesday, November 22, 2016   \n",
            "T 2016-11-22\n",
            "\u001b[92m☑\u001b[0m 2016-11-22\n",
            "---\n",
            "Q Saturday, July 18, 1970      \n",
            "T 1970-07-18\n",
            "\u001b[92m☑\u001b[0m 1970-07-18\n",
            "---\n",
            "Q october 6, 1992              \n",
            "T 1992-10-06\n",
            "\u001b[92m☑\u001b[0m 1992-10-06\n",
            "---\n",
            "Q 8/23/08                      \n",
            "T 2008-08-23\n",
            "\u001b[92m☑\u001b[0m 2008-08-23\n",
            "---\n",
            "Q 8/30/07                      \n",
            "T 2007-08-30\n",
            "\u001b[92m☑\u001b[0m 2007-08-30\n",
            "---\n",
            "Q 10/28/13                     \n",
            "T 2013-10-28\n",
            "\u001b[92m☑\u001b[0m 2013-10-28\n",
            "---\n",
            "Q sunday, november 6, 2016     \n",
            "T 2016-11-06\n",
            "\u001b[92m☑\u001b[0m 2016-11-06\n",
            "---\n",
            "val acc 100.000%\n",
            "| 에폭 6 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
            "| 에폭 6 |  반복 21 / 351 | 시간 12[s] | 손실 0.00\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}